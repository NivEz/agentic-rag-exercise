"""
Summary Pipeline Separation - Documentation
===========================================

Overview
--------
The summary generation process has been separated into its own independent pipeline.
This allows for more flexible workflow management:

1. pipeline.py: Handles PDF ingestion and chunking
2. summary_pipeline.py: Handles summary generation from existing chunks

Architecture
------------

BEFORE (Integrated):
PDF → Chunks → ChromaDB Chunks → Summaries → ChromaDB Summaries
     (all in one pipeline run)

AFTER (Separated):
Step 1: PDF → Chunks → ChromaDB Chunks (pipeline.py)
Step 2: ChromaDB Chunks → Summaries → ChromaDB Summaries (summary_pipeline.py)

Benefits of Separation
---------------------

1. **Flexibility**: Generate summaries independently of ingestion
2. **Selective Processing**: Generate summaries for specific claim IDs
3. **Re-generation**: Easily regenerate summaries with updated prompts
4. **Performance**: Skip summary generation during initial ingestion for speed
5. **Error Recovery**: If summary generation fails, chunks are already saved
6. **Incremental Updates**: Add summaries for new chunks without re-ingesting

File Changes
------------

NEW FILE: src/data_ingestion/summary_pipeline.py
- SummaryPipeline class for independent summary generation
- Reads chunks from ChromaDB chunks collection
- Generates summaries using configured LLM
- Stores summaries in ChromaDB summaries collection

MODIFIED: src/data_ingestion/pipeline.py
- Removed summary generation steps (Step 6-7)
- Removed summary-related imports and initialization
- Now focuses solely on PDF → Chunks workflow
- Returns simpler result without summary_ids

Usage Examples
--------------

### Workflow 1: Standard Two-Step Process

# Step 1: Ingest PDF and create chunks
python src/data_ingestion/pipeline.py --pdf "data/raw/claim.pdf" --claim-id "claim-123"

# Step 2: Generate summaries from chunks
python src/data_ingestion/summary_pipeline.py --claim-id "claim-123"

### Workflow 2: Batch Ingestion, Then Batch Summarization

# Ingest multiple PDFs first (fast)
python src/data_ingestion/pipeline.py --pdf "data/raw/claim1.pdf" --claim-id "claim-001"
python src/data_ingestion/pipeline.py --pdf "data/raw/claim2.pdf" --claim-id "claim-002"
python src/data_ingestion/pipeline.py --pdf "data/raw/claim3.pdf" --claim-id "claim-003"

# Generate all summaries at once
python src/data_ingestion/summary_pipeline.py

### Workflow 3: Check Status

# View statistics
python src/data_ingestion/summary_pipeline.py --stats

Command-Line Options
-------------------

### pipeline.py
- --pdf: Path to PDF file to process (required)
- --claim-id: Claim ID (optional, uses filename if not provided)
- --split-into-sections: Split markdown into sections before chunking
- --print-existing-chunks: Debug mode to print all chunks

### summary_pipeline.py
- --claim-id: Filter chunks by claim ID (optional, processes all if not provided)
- --stats: Print statistics about chunks and summaries

Features of summary_pipeline.py
-------------------------------

1. **Claim-Based Filtering**
   - Process summaries for specific claims
   - Useful for updating specific documents

2. **Progress Tracking**
   - Shows progress bar during generation
   - Reports statistics at completion

3. **Error Handling**
   - Continues processing if individual summaries fail
   - Creates fallback summaries for failed chunks
   - Reports errors without stopping pipeline

4. **Statistics View**
   - View chunk and summary counts
   - See claim ID distribution
   - Verify data without processing

Configuration
-------------

Both pipelines share the same config/config.yaml:

```yaml
llm:
  provider: "openai"
  model: "gpt-4o-mini"
  temperature: 0.0
  embedding_model: "text-embedding-3-small"

summarization:
  map_reduce: true
  summary_instruction: "Summarize this text with a focus on: 1. Key entities (people, organizations, locations). 2. A timeline of events (dates and actions). 3. Main ideas. Output a concise summary paragraph."
```

Migration Notes
---------------

If you were using the old integrated pipeline, no changes needed for ingestion.
Simply run summary_pipeline.py separately to generate summaries for existing chunks.

Example:
# Old way (integrated):
python src/data_ingestion/pipeline.py --pdf "claim.pdf"
# → Generated chunks AND summaries

# New way (separated):
python src/data_ingestion/pipeline.py --pdf "claim.pdf"
# → Generates chunks only

python src/data_ingestion/summary_pipeline.py
# → Generates summaries from chunks

Best Practices
--------------

1. **Initial Ingestion**: Use pipeline.py to ingest all PDFs first
2. **Batch Summarization**: Run summary_pipeline.py once for all chunks
3. **Incremental Updates**: New PDFs → pipeline.py, then summary_pipeline.py for just that claim
4. **Monitoring**: Periodically run --stats to verify data consistency

Troubleshooting
---------------

Q: Want to regenerate summaries with a new prompt?
A: Delete the summaries collection or ChromaDB database and re-run summary_pipeline.py

Q: Want to use different prompts for different claims?
A: Currently not supported - all summaries use same prompt from config

Q: How to delete old summaries?
A: Delete data/vector_store directory and re-run both pipelines

Q: Summary generation is slow?
A: Expected - each summary requires an LLM call. Consider using a faster model.
"""

