"""
Summary Index Implementation - Usage Guide
==========================================

Overview
--------
The summary index has been successfully integrated into the RAG pipeline. During PDF ingestion,
the system now:
1. Processes PDF into hierarchical chunks (as before)
2. Generates LLM-based summaries for all leaf chunks (new)
3. Stores chunks in ChromaDB 'chunks' collection
4. Stores summaries in ChromaDB 'summaries' collection

Architecture
------------
- Chunks Collection: Contains full hierarchical chunks for detailed retrieval
- Summaries Collection: Contains concise summaries optimized for high-level queries

Files Created/Modified
---------------------
NEW:
- src/data_ingestion/summary_generator.py: Core summarization logic

MODIFIED:
- src/data_ingestion/pipeline.py: 
  * Added LLM initialization in Settings
  * Added summary ChromaDB manager
  * Added Steps 6-7 for summary generation and storage
  
- src/utils/config_loader.py:
  * Added get_summarization_config()
  * Added get_llm_config()

Usage
-----
Run the ingestion pipeline normally:

```bash
python src/data_ingestion/pipeline.py --pdf "path/to/file.pdf" --claim-id "claim-123"
```

The pipeline will automatically:
1. Extract and chunk the PDF (Steps 1-5)
2. Generate summaries for leaf chunks (Step 6)
3. Store summaries in ChromaDB (Step 7)

Querying Summaries (Future Integration)
----------------------------------------
To use summaries in retrieval:

```python
from src.utils.vector_store import ChromaDBManager

# Initialize summary collection
summary_manager = ChromaDBManager(
    persist_directory="./data/vector_store",
    collection_name="summaries",
    embedding_model_name="text-embedding-3-small"
)

# Get index and query
index = summary_manager.get_index()
query_engine = index.as_query_engine(similarity_top_k=5)

# Query for high-level information
response = query_engine.query("What are the key events in this claim?")
```

Metadata in Summaries
--------------------
Each summary node contains:
- is_summary: True (identifies as summary)
- source_chunk_id: Link to original chunk
- chunk_level: Granularity level (from HierarchicalNodeParser metadata)
- claim_id: Claim identifier
- source_file: Original PDF filename
- source_path: Full path to PDF

Benefits
--------
1. Fast Summary Queries: Pre-computed summaries avoid re-processing
2. Reduced Token Usage: Summaries are more concise than full chunks
3. Timeline Extraction: LLM focuses on events, dates, and key details
4. Agent-Ready: Summaries collection can be used by summary-focused agents
5. Flexible Retrieval: Choose chunks for detail, summaries for overview

Test Results
-----------
✓ Successfully tested with 'Insurance Claim Report 1-2.pdf'
✓ Generated 13 summaries from 13 leaf chunks
✓ Both collections properly populated in ChromaDB
✓ Metadata correctly preserved and enhanced
✓ Summaries contain focused, concise information

Configuration
-------------
Configure in config/config.yaml:

summarization:
  map_reduce: true
  summary_instruction: "Your custom instruction here"

llm:
  model: "gpt-4o-mini"
  temperature: 0.0
  embedding_model: "text-embedding-3-small"
"""

